*** dm-cache.c	2012-04-02 19:38:18.551766137 -0400
--- dm-cache.c.orgi	2012-04-02 18:57:28.579766095 -0400
***************
*** 3,14 ****
   *  Device mapper target for block-level disk caching
   *
   *  Copyright (C) International Business Machines Corp., 2006
!  *  Copyright (C) Ming Zhao, Florida International University, 2007-2012
   *
!  *  Authors: Ming Zhao, Dulcardo Arteaga, Douglas Otstott, Stephen Bromfield
!  *           (dm-cache@googlegroups.com)
!  *  Other contributors:
!  *    Eric Van Hensbergen, Reng Zeng 
   *
   *  This program is free software; you can redistribute it and/or modify
   *  it under the terms of the GNU General Public License as published by
--- 3,15 ----
   *  Device mapper target for block-level disk caching
   *
   *  Copyright (C) International Business Machines Corp., 2006
!  *  Copyright (C) Ming Zhao, Florida International University, 2008-2009
   *
!  *  Author: Ming Zhao (mingzhao99th@gmail.com)
!  *  Contributors:
!  *    Eric Van Hensbergen (metoring of this intern project at IBM in 2006)
!  *    Reng Zeng (code update for 2.6.27-kenrel as part of his COP6611 course
!  *    project at FIU in 2008)
   *
   *  This program is free software; you can redistribute it and/or modify
   *  it under the terms of the GNU General Public License as published by
***************
*** 40,58 ****
  #include <linux/dm-io.h>
  #include <linux/dm-kcopyd.h>
  
! #define DMC_DEBUG 0
  
  #define DM_MSG_PREFIX "cache"
  #define DMC_PREFIX "dm-cache: "
  
  #if DMC_DEBUG
  #define DPRINTK( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
- #define VALSECTOR( w, x, y, z ) validate_sector(w,x,y,z)	/* validate bio store */
- #define VALFETCH( x, y, z  ) validate_fetch(x,y,z)		/* validate bio fetch */
  #else
  #define DPRINTK( s, arg... )
- #define VALSECTOR( w, x, y, z ) 
- #define VALFETCH( x, y, z ) 
  #endif
  
  /* Default cache parameters */
--- 41,55 ----
  #include <linux/dm-io.h>
  #include <linux/dm-kcopyd.h>
  
! #define DMC_DEBUG 1
  
  #define DM_MSG_PREFIX "cache"
  #define DMC_PREFIX "dm-cache: "
  
  #if DMC_DEBUG
  #define DPRINTK( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
  #else
  #define DPRINTK( s, arg... )
  #endif
  
  /* Default cache parameters */
***************
*** 75,129 ****
  #define RESERVED	2	/* Allocated but data not in place yet */
  #define DIRTY		4	/* Locally modified */
  #define WRITEBACK	8	/* In the process of write back */
- #define WRITETHROUGH	16	/* In the process of write through */
- #define READY           0
- #define PENDING_WRITE   1	
- #define PENDING_READ    2
- 
- #define HASH 0         		/* Use hash_long */
- #define UNIFORM 1     		/* Evenly distributed */
- #define DEFAULT_HASHFUNC HASH
  
  #define is_state(x, y)		(x & y)
  #define set_state(x, y)		(x |= y)
  #define clear_state(x, y)	(x &= ~y)
  
  /*
-  * Validations
-  */
- #define SEG_SIZE_ORDER  0 
- #define SEG_SIZE_BYTES  512
- 
- /*
- * Virtual Cache Mapping
- */
- #define MAX_SRC_DEVICES		128
- 
- int 	idx_dm_dev = 0;		/* keep the number of current mappings */
- int 	ctn_dm_dev = 0;		/* keep the number of total mappings   */
- int	init_flag = 0;		/* use to determine the fisrt mapping */
- 
- /*
-  * Cache mappings
-  */
- struct v_map {
- 	int		identifier;	/* virtual machine mapping */
- 	int		active;		
- 	dev_t 		vcache_dev;
- 
- 	sector_t 	dev_size;
- 	sector_t 	dev_offset;
- 
- 	struct dm_dev 	*src_dev;
- 	struct dm_target *ti;
- };
- 
- 
- /*
   * Cache context
   */
  struct cache_c {
- 	struct dm_target *global_ti;	/* global dm_target to hold global cache */	
  	struct dm_dev *src_dev;		/* Source device */
  	struct dm_dev *cache_dev;	/* Cache device */
  	struct dm_kcopyd_client *kcp_client; /* Kcopyd client for writing back data */
--- 72,86 ----
***************
*** 164,171 ****
  	unsigned short state;	/* State of a block */
  	unsigned long counter;	/* Logical timestamp of the block's last access */
  	struct bio_list bios;	/* List of pending bios */
- 	int	disk;		/* Disk identifier for LV of each VM */
- 	atomic_t status;
  };
  
  /* Structure for a kcached job */
--- 121,126 ----
***************
*** 177,183 ****
  	struct dm_io_region dest;
  	struct cacheblock *cacheblock;
  	int rw;
- 	int vdisk;
  	/*
  	 * When the original bio is not aligned with cache blocks,
  	 * we need extra bvecs and pages for padding.
--- 132,137 ----
***************
*** 187,206 ****
  	struct page_list *pages;
  };
  
- /*****************************************************************
- *	Shared structures
- ******************************************************************/
- struct cache_c *shared_cache;
- struct v_map *virtual_mapping;
- 
- /*****************************************************************
- *	Functions
- ******************************************************************/
- static int validate_sector(sector_t sector_source, sector_t sector_cache,struct cache_c *dmc,int vdisk);
- static int validate_fetch(sector_t sector_source, struct page *fetch_page,int vdisk);
- static int do_bio_read(struct block_device *bi_bdev, sector_t block,struct page *page_read);
- 
- static int virtual_cache_map(struct bio *bio);
  
  /****************************************************************************
   *  Wrapper functions for using the new dm_io API
--- 141,146 ----
***************
*** 614,620 ****
  static int do_store(struct kcached_job *job)
  {
  	int i, j, r = 0;
! 	struct bio *bio = job->bio ;
  	struct cache_c *dmc = job->dmc;
  	unsigned int offset, head, tail, remaining, nr_vecs;
  	struct bio_vec *bvec;
--- 550,556 ----
  static int do_store(struct kcached_job *job)
  {
  	int i, j, r = 0;
! 	struct bio *bio = job->bio, *clone;
  	struct cache_c *dmc = job->dmc;
  	unsigned int offset, head, tail, remaining, nr_vecs;
  	struct bio_vec *bvec;
***************
*** 637,644 ****
  	   each of them to prevent the pages being freed before the cache
  	   insertion is completed.
  	 */
! 	if (bio_data_dir(bio) == READ) 
! 			VALFETCH(job->src.sector,bio->bi_io_vec[0].bv_page,job->vdisk);
  
  	if (0 == job->nr_pages) /* Original request is aligned with cache blocks */
  		r = dm_io_async_bvec(1, &job->dest, WRITE, bio->bi_io_vec + bio->bi_idx,
--- 570,585 ----
  	   each of them to prevent the pages being freed before the cache
  	   insertion is completed.
  	 */
! 	if (bio_data_dir(bio) == READ) {
! 		clone = bio_clone(bio, GFP_NOIO);
! 		for (i=bio->bi_idx; i<bio->bi_vcnt; i++) {
! 			get_page(bio->bi_io_vec[i].bv_page);
! 		}
! 		DPRINTK("bio ended for %llu:%u", bio->bi_sector, bio->bi_size);
! 		bio_endio(bio, 0);
! 		bio = clone;
! 		job->bio = clone;
! 	}
  
  	if (0 == job->nr_pages) /* Original request is aligned with cache blocks */
  		r = dm_io_async_bvec(1, &job->dest, WRITE, bio->bi_io_vec + bio->bi_idx,
***************
*** 731,738 ****
  	bio = bio_list_get(&cacheblock->bios);
  	if (is_state(cacheblock->state, WRITEBACK)) { /* Write back finished */
  		cacheblock->state = VALID;
- 	} else if (is_state(cacheblock->state, WRITETHROUGH)) { 
- 		cacheblock->state = INVALID;
  	} else { /* Cache insertion finished */
  		set_state(cacheblock->state, VALID);
  		clear_state(cacheblock->state, RESERVED);
--- 672,677 ----
***************
*** 752,763 ****
  
  static int do_complete(struct kcached_job *job)
  {
! 	int r = 0;
  	struct bio *bio = job->bio;
  
  	DPRINTK("do_complete: %llu", bio->bi_sector);
  
! 	VALSECTOR (job->src.sector,job->dest.sector,job->dmc, job->vdisk);
  	bio_endio(bio, 0);
  
  	if (job->nr_pages > 0) {
--- 690,706 ----
  
  static int do_complete(struct kcached_job *job)
  {
! 	int i, r = 0;
  	struct bio *bio = job->bio;
  
  	DPRINTK("do_complete: %llu", bio->bi_sector);
  
! 	if (bio_data_dir(bio) == READ) {
! 		for (i=bio->bi_idx; i<bio->bi_vcnt; i++) {
! 			put_page(bio->bi_io_vec[i].bv_page);
! 		}
! 		bio_put(bio);
! 	} else
  		bio_endio(bio, 0);
  
  	if (job->nr_pages > 0) {
***************
*** 766,774 ****
  	}
  
  	flush_bios(job->cacheblock);
- 
- 	atomic_set(&job->cacheblock->status, 0);
- 
  	mempool_free(job, _job_pool);
  
  	if (atomic_dec_and_test(&job->dmc->nr_jobs))
--- 709,714 ----
***************
*** 918,932 ****
  
         value = (unsigned long)(block >> (dmc->block_shift +
                 dmc->consecutive_shift));
-        if (DEFAULT_HASHFUNC == HASH)
                 set_number = hash_long(value, dmc->bits) / dmc->assoc;
!        else if(DEFAULT_HASHFUNC == UNIFORM)
!                set_number = value & ((unsigned long)(dmc->size >>
!                         dmc->consecutive_shift) - 1);
! 
!        DPRINTK("Hash: %llu(%lu)->%lu", (long long unsigned int)block, 
! 		       (long unsigned int)value, 
! 		       (long unsigned int)set_number);
         return set_number;
  }
  
--- 854,861 ----
  
  	value = (unsigned long)(block >> (dmc->block_shift +
  	        dmc->consecutive_shift));
  	set_number = hash_long(value, dmc->bits) / dmc->assoc;
! 
   	return set_number;
  }
  
***************
*** 948,1076 ****
  	dmc->counter = 0;
  }
  
- 
- static void req_endio(struct bio *bio, int err)
- {
-         int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);
-         BUG_ON(!uptodate);
- 
-         if(bio->bi_private)
-                 complete(bio->bi_private);
- }
- 
- static int do_bio_read(struct block_device *bi_bdev, sector_t block,struct page *page_read)
- {
- 	struct bio *bio;
- 	int req_size = 512;
- 	struct page *page;
- 
- 	DECLARE_COMPLETION(comp);
- 
- 	bio = bio_alloc(GFP_KERNEL | __GFP_NOFAIL, BIO_MAX_PAGES);
- 	if(bio == NULL){
- 		DPRINTK("Bio not allocated");
- 		return -1;
- 	}
- 
- 	bio->bi_sector = block;
- 	bio->bi_bdev = bi_bdev;
- 	bio->bi_end_io = req_endio;
- 	bio->bi_rw = 0;
- 	bio->bi_private = &comp;
- 
- 	/* allocate contigous pages */
- 	page = alloc_page(GFP_KERNEL );
- 	if(page == NULL) {
- 		DPRINTK("Page not allocated");
- 		return -ENOMEM;
- 	}
- 
- 	/* fill all pages with 0 */
- 	memset(kmap(page),0,SEG_SIZE_BYTES);
- 	bio_add_page(bio, page, SEG_SIZE_BYTES, 0);
- 
- 	/* submit the bio */
- 	generic_make_request(bio);
- 	wait_for_completion(&comp);
- 
- 	memcpy(kmap(page_read),kmap(bio->bi_io_vec[0].bv_page),req_size);
- 	__free_page(bio->bi_io_vec[0].bv_page);
- 	bio_put(bio);
- 
- 	return req_size;
- }
- 
- static int validate_sector(sector_t sector_source, sector_t sector_cache,struct cache_c *dmc,int vdisk)
- {
-         int size = 512;
-         struct page *cache_page,*source_page;
- 
-         struct block_device *cache_dev = dmc->cache_dev->bdev;
-         struct block_device *source_dev =  virtual_mapping[vdisk].src_dev->bdev;
- 
-         cache_page = alloc_page(GFP_KERNEL );
-         if(!cache_page) return -ENOMEM;
-         memset(kmap(cache_page),0,SEG_SIZE_BYTES);
- 
-         source_page = alloc_page(GFP_KERNEL);
-         if(!source_page) return -ENOMEM;
-         memset(kmap(source_page),0,SEG_SIZE_BYTES);
- 
-         do_bio_read(cache_dev,sector_cache,cache_page);
-         do_bio_read(source_dev,sector_source,source_page);
- 
-         if(0==memcmp(kmap(cache_page),kmap(source_page),size)){
-                 DPRINTK("STORE block %llu -> %llu are EQUALS\n",
- 				(long long unsigned int)sector_cache, 
- 				(long long unsigned int)sector_source);
-         }else{
-                 DPRINTK("STORE block %llu -> %llu are DIFFERENT\n",
- 				(long long unsigned int)sector_cache, 
- 				(long long unsigned int)sector_source);
-         }
- 	kunmap(cache_page);
- 	kunmap(source_page);
-         __free_page(cache_page);
-         __free_page(source_page);
- 
-         return 0;
- }
- static int validate_fetch(sector_t sector_source, struct page *fetch_page, int vdisk)
- {
-         int size = 512;
-         struct page *source_page;
- 
-         struct block_device *source_dev =  virtual_mapping[vdisk].src_dev->bdev;
- 
-         source_page = alloc_page(GFP_KERNEL);
-         if(!source_page) return -ENOMEM;
- 	
- 	if(kmap(source_page)==NULL){ 
- 		DPRINTK("KMAP NULL");
- 		return -ENOMEM;
- 	}
- 	if(kmap(fetch_page)==NULL){ 
- 		DPRINTK("KMAP NULL");
- 		return -ENOMEM;
- 	}
- 	
-         memset(kmap(source_page),0,SEG_SIZE_BYTES);
- 
-         do_bio_read(source_dev,sector_source,source_page);
- 
- 	
-         if(0==memcmp(kmap(fetch_page),kmap(source_page),size)){
-                 DPRINTK("FETCH block %llu are EQUALS\n",(long long unsigned int)sector_source);
-         }else{
-                 DPRINTK("FETCH block %llu are DIFFERENT\n",(long long unsigned int)sector_source);
-         }
- 	kunmap(fetch_page);
- 	kunmap(source_page);
-         __free_page(source_page);
- 
-         return 0;
- }
- 
  /*
   * Lookup a block in the cache.
   *
--- 877,882 ----
***************
*** 1089,1117 ****
   *
   */
  static int cache_lookup(struct cache_c *dmc, sector_t block,
!                             sector_t *cache_block, int disk)
  {
!         unsigned long set_number;
! 	sector_t __block, index, block_ori;
          int i, res;
          unsigned int cache_assoc  = dmc->assoc;
          struct cacheblock *cache = dmc->cache;
          int invalid = -1, oldest = -1, oldest_clean = -1;
          unsigned long counter = ULONG_MAX, clean_counter = ULONG_MAX;
  
-         block_ori = block;
-         __block = block_ori + virtual_mapping[disk].dev_offset;
- 
-         set_number = hash_block(dmc, __block);
- 
  	index=set_number * cache_assoc;
  
          for (i=0; i<cache_assoc; i++, index++) {
  		if (is_state(cache[index].state, VALID) ||
                      is_state(cache[index].state, RESERVED)) {
! 
!                         if (cache[index].block == block_ori && 
! 				cache[index].disk == disk) {
                                  *cache_block = index;
                                  /* Reset all counters if the largest one is going to overflow */
                                  if (dmc->counter == ULONG_MAX) cache_reset_counter(dmc);
--- 895,916 ----
   *
   */
  static int cache_lookup(struct cache_c *dmc, sector_t block,
! 	                    sector_t *cache_block)
  {
! 	unsigned long set_number = hash_block(dmc, block);
! 	sector_t index;
  	int i, res;
  	unsigned int cache_assoc = dmc->assoc;
  	struct cacheblock *cache = dmc->cache;
  	int invalid = -1, oldest = -1, oldest_clean = -1;
  	unsigned long counter = ULONG_MAX, clean_counter = ULONG_MAX;
  
  	index=set_number * cache_assoc;
  
  	for (i=0; i<cache_assoc; i++, index++) {
  		if (is_state(cache[index].state, VALID) ||
  		    is_state(cache[index].state, RESERVED)) {
! 			if (cache[index].block == block) {
  				*cache_block = index;
  				/* Reset all counters if the largest one is going to overflow */
  				if (dmc->counter == ULONG_MAX) cache_reset_counter(dmc);
***************
*** 1165,1178 ****
   * Insert a block into the cache (in the frame specified by cache_block).
   */
  static int cache_insert(struct cache_c *dmc, sector_t block,
! 	                    sector_t cache_block, int disk)
  {
  	struct cacheblock *cache = dmc->cache;
  
  	/* Mark the block as RESERVED because although it is allocated, the data are
         not in place until kcopyd finishes its job.
  	 */
- 	cache[cache_block].disk = disk;
  	cache[cache_block].block = block;
  	cache[cache_block].state = RESERVED;
  	if (dmc->counter == ULONG_MAX) cache_reset_counter(dmc);
--- 964,976 ----
   * Insert a block into the cache (in the frame specified by cache_block).
   */
  static int cache_insert(struct cache_c *dmc, sector_t block,
! 	                    sector_t cache_block)
  {
  	struct cacheblock *cache = dmc->cache;
  
  	/* Mark the block as RESERVED because although it is allocated, the data are
         not in place until kcopyd finishes its job.
  	 */
  	cache[cache_block].block = block;
  	cache[cache_block].state = RESERVED;
  	if (dmc->counter == ULONG_MAX) cache_reset_counter(dmc);
***************
*** 1228,1242 ****
  		return 0;
  	} else { /* WRITE hit */
  		if (dmc->write_policy == WRITE_THROUGH) { /* Invalidate cached data */
- 			if (is_state(cache[cache_block].state, VALID)) {
  				cache_invalidate(dmc, cache_block);
! 				bio->bi_bdev = virtual_mapping[virtual_cache_map(bio)].src_dev->bdev;
  				return 1;
  			}
- 				set_state(cache[cache_block].state,WRITETHROUGH);
- 				bio_list_add(&cache[cache_block].bios, bio);
- 				return 0;
- 		}
  
  		/* Write delay */
  		if (!is_state(cache[cache_block].state, DIRTY)) {
--- 1026,1035 ----
  		return 0;
  	} else { /* WRITE hit */
  		if (dmc->write_policy == WRITE_THROUGH) { /* Invalidate cached data */
  			cache_invalidate(dmc, cache_block);
! 			bio->bi_bdev = dmc->src_dev->bdev;
  			return 1;
  		}
  
  		/* Write delay */
  		if (!is_state(cache[cache_block].state, DIRTY)) {
***************
*** 1249,1258 ****
   		/* In the middle of write back */
  		if (is_state(cache[cache_block].state, WRITEBACK)) {
  			/* Delay this write until the block is written back */
! 			bio->bi_bdev = virtual_mapping[virtual_cache_map(bio)].src_dev->bdev;
  			DPRINTK("Add to bio list %s(%llu)",
! 					virtual_mapping[virtual_cache_map(bio)].src_dev->name,
! 					(long long unsigned int)bio->bi_sector);
  			bio_list_add(&cache[cache_block].bios, bio);
  			spin_unlock(&cache[cache_block].lock);
  			return 0;
--- 1042,1050 ----
   		/* In the middle of write back */
  		if (is_state(cache[cache_block].state, WRITEBACK)) {
  			/* Delay this write until the block is written back */
! 			bio->bi_bdev = dmc->src_dev->bdev;
  			DPRINTK("Add to bio list %s(%llu)",
! 					dmc->src_dev->name, bio->bi_sector);
  			bio_list_add(&cache[cache_block].bios, bio);
  			spin_unlock(&cache[cache_block].lock);
  			return 0;
***************
*** 1284,1292 ****
  {
  	struct dm_io_region src, dest;
  	struct kcached_job *job;
- 	int pos = virtual_cache_map(bio);
  
! 	src.bdev = virtual_mapping[pos].src_dev->bdev; ;
  	src.sector = request_block;
  	src.count = dmc->block_size;
  	dest.bdev = dmc->cache_dev->bdev;
--- 1076,1083 ----
  {
  	struct dm_io_region src, dest;
  	struct kcached_job *job;
  
! 	src.bdev = dmc->src_dev->bdev;
  	src.sector = request_block;
  	src.count = dmc->block_size;
  	dest.bdev = dmc->cache_dev->bdev;
***************
*** 1298,1304 ****
  	job->bio = bio;
  	job->src = src;
  	job->dest = dest;
- 	job->vdisk = pos;
  	job->cacheblock = &dmc->cache[cache_block];
  
  	return job;
--- 1089,1094 ----
***************
*** 1310,1316 ****
   *  store data to cache device.
   */
  static int cache_read_miss(struct cache_c *dmc, struct bio* bio,
! 	                       sector_t cache_block, int disk) {
  	struct cacheblock *cache = dmc->cache;
  	unsigned int offset, head, tail;
  	struct kcached_job *job;
--- 1100,1106 ----
   *  store data to cache device.
   */
  static int cache_read_miss(struct cache_c *dmc, struct bio* bio,
! 	                       sector_t cache_block) {
  	struct cacheblock *cache = dmc->cache;
  	unsigned int offset, head, tail;
  	struct kcached_job *job;
***************
*** 1326,1338 ****
          } else DPRINTK("Insert block %llu at empty frame %llu",
                  request_block, cache_block);
  
! 	cache_insert(dmc, request_block, cache_block, disk); /* Update metadata first */
  
  	job = new_kcached_job(dmc, bio, request_block, cache_block);
  
  	head = to_bytes(offset);
  
! 	left = (virtual_mapping[virtual_cache_map(bio)].src_dev->bdev->bd_inode->i_size>>9) - request_block;
  	if (left < dmc->block_size) {
  		tail = to_bytes(left) - bio->bi_size - head;
  		job->src.count = left;
--- 1116,1128 ----
  	} else DPRINTK("Insert block %llu at empty frame %llu",
  		request_block, cache_block);
  
! 	cache_insert(dmc, request_block, cache_block); /* Update metadata first */
  
  	job = new_kcached_job(dmc, bio, request_block, cache_block);
  
  	head = to_bytes(offset);
  
! 	left = (dmc->src_dev->bdev->bd_inode->i_size>>9) - request_block;
  	if (left < dmc->block_size) {
  		tail = to_bytes(left) - bio->bi_size - head;
  		job->src.count = left;
***************
*** 1349,1368 ****
  
          DPRINTK("Queue job for %llu (need %u pages)",
                  bio->bi_sector, job->nr_pages);
- 
- 	while (1) {
- 		set_current_state(TASK_UNINTERRUPTIBLE);
- 
- 		if (!atomic_read(&cache[cache_block].status))
- 			break;
- 		DPRINTK("WAIT for %llu - %llu ",
- 				(long long unsigned int)cache_block,
- 				(long long unsigned int)request_block);
- 		io_schedule();
- 	}
- 	atomic_set(&cache[cache_block].status,1);
- 	set_current_state(TASK_RUNNING);
- 
  	queue_job(job);
  
  	return 0;
--- 1139,1144 ----
***************
*** 1374,1387 ****
   *  If write-back, update the metadata; fetch the necessary block from source
   *  device; write to cache device.
   */
! static int cache_write_miss(struct cache_c *dmc, struct bio* bio, sector_t cache_block,int disk) {
  	struct cacheblock *cache = dmc->cache;
  	unsigned int offset, head, tail;
  	struct kcached_job *job;
  	sector_t request_block, left;
  
  	if (dmc->write_policy == WRITE_THROUGH) { /* Forward request to souuce */
! 		bio->bi_bdev = virtual_mapping[virtual_cache_map(bio)].src_dev->bdev;
  		return 1;
  	}
  
--- 1150,1163 ----
   *  If write-back, update the metadata; fetch the necessary block from source
   *  device; write to cache device.
   */
! static int cache_write_miss(struct cache_c *dmc, struct bio* bio, sector_t cache_block) {
  	struct cacheblock *cache = dmc->cache;
  	unsigned int offset, head, tail;
  	struct kcached_job *job;
  	sector_t request_block, left;
  
  	if (dmc->write_policy == WRITE_THROUGH) { /* Forward request to souuce */
! 		bio->bi_bdev = dmc->src_dev->bdev;
  		return 1;
  	}
  
***************
*** 1396,1409 ****
                  request_block, cache_block);
  
  	/* Write delay */
! 	cache_insert(dmc, request_block, cache_block,disk); /* Update metadata first */
  	set_state(cache[cache_block].state, DIRTY);
  	dmc->dirty_blocks++;
  
  	job = new_kcached_job(dmc, bio, request_block, cache_block);
  
  	head = to_bytes(offset);
! 	left = (virtual_mapping[virtual_cache_map(bio)].src_dev->bdev->bd_inode->i_size>>9) - request_block;
  	if (left < dmc->block_size) {
  		tail = to_bytes(left) - bio->bi_size - head;
  		job->src.count = left;
--- 1172,1185 ----
  		request_block, cache_block);
  
  	/* Write delay */
! 	cache_insert(dmc, request_block, cache_block); /* Update metadata first */
  	set_state(cache[cache_block].state, DIRTY);
  	dmc->dirty_blocks++;
  
  	job = new_kcached_job(dmc, bio, request_block, cache_block);
  
  	head = to_bytes(offset);
! 	left = (dmc->src_dev->bdev->bd_inode->i_size>>9) - request_block;
  	if (left < dmc->block_size) {
  		tail = to_bytes(left) - bio->bi_size - head;
  		job->src.count = left;
***************
*** 1435,1461 ****
  }
  
  /* Handle cache misses */
! static int cache_miss(struct cache_c *dmc, struct bio* bio, sector_t cache_block, int disk) {
  	if (bio_data_dir(bio) == READ)
! 		return cache_read_miss(dmc, bio, cache_block,disk);
  	else
! 		return cache_write_miss(dmc, bio, cache_block,disk);
! }
! 
! static int virtual_cache_map(struct bio *bio)
! {
! 	int i = 0, ret = -1;
! 
! 	for (i=0; i < ctn_dm_dev ; i++)
! 	{
! 		if(virtual_mapping[i].vcache_dev == bio->bi_bdev->bd_dev 
! 				&& virtual_mapping[i].active == 1)
! 			ret = i;
! 	}
! 	if(ret == -1)
! 		DMERR("Virtual cache not found %llu",
! 				(long long unsigned int)bio->bi_bdev->bd_dev);
! 	return ret;
  }
  
  
--- 1211,1221 ----
  }
  
  /* Handle cache misses */
! static int cache_miss(struct cache_c *dmc, struct bio* bio, sector_t cache_block) {
  	if (bio_data_dir(bio) == READ)
! 		return cache_read_miss(dmc, bio, cache_block);
  	else
! 		return cache_write_miss(dmc, bio, cache_block);
  }
  
  
***************
*** 1469,1479 ****
  static int cache_map(struct dm_target *ti, struct bio *bio,
  		      union map_info *map_context)
  {
! 	struct cache_c *dmc = shared_cache;
! 
  	sector_t request_block, cache_block = 0, offset;
  	int res;
- 	int disk;
  
  	offset = bio->bi_sector & dmc->block_mask;
  	request_block = bio->bi_sector - offset;
--- 1229,1237 ----
  static int cache_map(struct dm_target *ti, struct bio *bio,
  		      union map_info *map_context)
  {
! 	struct cache_c *dmc = (struct cache_c *) ti->private;
  	sector_t request_block, cache_block = 0, offset;
  	int res;
  
  	offset = bio->bi_sector & dmc->block_mask;
  	request_block = bio->bi_sector - offset;
***************
*** 1482,1505 ****
                  bio_rw(bio) == WRITE ? "WRITE" : (bio_rw(bio) == READ ?
                  "READ":"READA"), bio->bi_sector, request_block, offset,
                  bio->bi_size);
  
  	if (bio_data_dir(bio) == READ) dmc->reads++;
  	else dmc->writes++;
  
! 	disk = virtual_cache_map(bio);
! 
! 	res = cache_lookup(dmc, request_block, &cache_block,disk);
  	if (1 == res)  /* Cache hit; server request from cache */
  		return  cache_hit(dmc, bio, cache_block);
  	else if (0 == res) /* Cache miss; replacement block is found */
! 		return  cache_miss(dmc, bio, cache_block,disk);
  	else if (2 == res) { /* Entire cache set is dirty; initiate a write-back */
  		write_back(dmc, cache_block, 1);
  		dmc->writeback++;
  	}
  
  	/* Forward to source device */
! 	bio->bi_bdev = virtual_mapping[virtual_cache_map(bio)].src_dev->bdev;
  
  	return 1;
  }
--- 1240,1262 ----
  	        bio_rw(bio) == WRITE ? "WRITE" : (bio_rw(bio) == READ ?
  	        "READ":"READA"), bio->bi_sector, request_block, offset,
  	        bio->bi_size);
+ 	DPRINTK("SOURCE: %llu",bio->bi_bdev->bd_dev);
  
  	if (bio_data_dir(bio) == READ) dmc->reads++;
  	else dmc->writes++;
  
! 	res = cache_lookup(dmc, request_block, &cache_block);
  	if (1 == res)  /* Cache hit; server request from cache */
  		return cache_hit(dmc, bio, cache_block);
  	else if (0 == res) /* Cache miss; replacement block is found */
! 		return cache_miss(dmc, bio, cache_block);
  	else if (2 == res) { /* Entire cache set is dirty; initiate a write-back */
  		write_back(dmc, cache_block, 1);
  		dmc->writeback++;
  	}
  
  	/* Forward to source device */
! 	bio->bi_bdev = dmc->src_dev->bdev;
  
  	return 1;
  }
***************
*** 1687,1701 ****
  	return 0;
  }
  
- static sector_t calculate_offset( int num_devices )
- {
- 	DPRINTK("start Calculate offset: %d",num_devices);
- 	if(num_devices == 0)
- 		return 0;
- 	else
- 		return (virtual_mapping[num_devices-1].dev_offset + virtual_mapping[num_devices-1].dev_size);
- }
- 
  /*
   * Construct a cache mapping.
   *  arg[0]: path to source device
--- 1443,1448 ----
***************
*** 1715,1820 ****
  	sector_t data_size, meta_size, dev_size;
  	unsigned long long cache_size;
  	int r = -EINVAL;
- 	struct mapped_device *mapped_dev;
- 	struct dm_dev *virtual_cache;
- 	struct dm_dev_internal *dd;
  
  	if (argc < 2) {
  		ti->error = "dm-cache: Need at least 2 arguments (src dev and cache dev)";
  		goto bad;
  	}
  
! 	/* This is the first time a mapping is created */
! 	if(init_flag == 0){
! 		shared_cache = kmalloc(sizeof(*shared_cache), GFP_KERNEL);
! 		virtual_mapping = kmalloc(sizeof(*virtual_mapping)*MAX_SRC_DEVICES , GFP_KERNEL);
! 		if (virtual_mapping == NULL){
! 			ti->error = "dm-cache: Failed to allocate cache context";
! 			r = ENOMEM;
! 			goto bad;
! 		}
! 	}
! 
! 	dmc = shared_cache;
  	if (dmc == NULL) {
  		ti->error = "dm-cache: Failed to allocate cache context";
  		r = ENOMEM;
  		goto bad;
  	}
  
! 	/*  Adding source device */
! 	r = dm_get_device(ti, argv[0],
! 			dm_table_get_mode(ti->table), &virtual_mapping[ctn_dm_dev].src_dev);
! 	virtual_mapping[ctn_dm_dev].ti = ti;
  	if (r) {
  		ti->error = "dm-cache: Source device lookup failed";
  		goto bad1;
- 	}else {
- 		DPRINTK("Registering device %s:%llu",virtual_mapping[ctn_dm_dev].src_dev->name,
- 				(long long unsigned int)virtual_mapping[ctn_dm_dev].src_dev->bdev->bd_dev);
  	}
  
! 	/* Calculate Size and offset of the device */
! 	virtual_mapping[ctn_dm_dev].identifier = ctn_dm_dev;
! 	virtual_mapping[ctn_dm_dev].dev_size = ti->len;
! 	virtual_mapping[ctn_dm_dev].dev_offset = calculate_offset(ctn_dm_dev );
! 
! 	/* Adding virtual cache devices */
! 	mapped_dev = dm_table_get_md(ti->table);
! 	r = dm_get_device(ti, dm_device_name(mapped_dev),
! 			dm_table_get_mode(ti->table), &virtual_cache);
! 	if (r) {
! 		ti->error = "dm-cache: virtual cache device lookup failed";
! 		goto bad1;
! 	}else {
! 		virtual_mapping[ctn_dm_dev].vcache_dev = virtual_cache->bdev->bd_dev;
! 
! 		DPRINTK("Registering %d device %s:%llu",
! 				ctn_dm_dev,
! 				dm_device_name(mapped_dev),
! 				(long long unsigned int)virtual_mapping[ctn_dm_dev].vcache_dev);
! 		dm_put_device(ti, virtual_cache);
! 		virtual_mapping[ctn_dm_dev].active = 1;
! 		ctn_dm_dev++;
! 		idx_dm_dev++;
! 	}
! 
! 
! 	/* Adding global cache device */
! 	if(init_flag == 0) {
! 		DPRINTK("Registering %s",argv[1]);
! 		dd =  kmalloc(sizeof(*dd), GFP_KERNEL);
! 		dd->dm_dev.mode = dm_table_get_mode(ti->table);		
! 		dd->dm_dev.bdev = lookup_bdev(argv[1]);
! 
! 		r = blkdev_get( dd->dm_dev.bdev , dd->dm_dev.mode, NULL);
  		if (r) {
  			ti->error = "dm-cache: Cache device lookup failed";
- 			kfree(dd);
  			goto bad2;
- 		}else{
- 
- 			format_dev_t(dd->dm_dev.name, dd->dm_dev.bdev->bd_dev);
- 			atomic_set(&dd->count, 0);
- 			atomic_inc(&dd->count);
- 
- 			dmc->cache_dev = &dd->dm_dev;
- 
- 			DPRINTK("Registering device %s:%llu",argv[1],
- 					(long long unsigned int)dmc->cache_dev->bdev->bd_dev);
- 		}
  	}	
  
! 	if(init_flag == 0) {
! 		dmc->io_client = dm_io_client_create();
  		if (IS_ERR(dmc->io_client)) {
  			r = PTR_ERR(dmc->io_client);
  			ti->error = "Failed to create io client\n";
  			goto bad3;
  		}
  
! 		dmc->kcp_client = dm_kcopyd_client_create();
! 		if (dmc->kcp_client == NULL) {
  			ti->error = "Failed to initialize kcopyd client\n";
  			goto bad4;
  		}
--- 1462,1503 ----
  	sector_t data_size, meta_size, dev_size;
  	unsigned long long cache_size;
  	int r = -EINVAL;
  
  	if (argc < 2) {
  		ti->error = "dm-cache: Need at least 2 arguments (src dev and cache dev)";
  		goto bad;
  	}
  
! 	dmc = kmalloc(sizeof(*dmc), GFP_KERNEL);
  	if (dmc == NULL) {
  		ti->error = "dm-cache: Failed to allocate cache context";
  		r = ENOMEM;
  		goto bad;
  	}
  
! 	r = dm_get_device(ti, argv[0],0,0,
! 			  dm_table_get_mode(ti->table), &dmc->src_dev);
  	if (r) {
  		ti->error = "dm-cache: Source device lookup failed";
  		goto bad1;
  	}
  
! 	r = dm_get_device(ti, argv[1],0,0,
! 			  dm_table_get_mode(ti->table), &dmc->cache_dev);
  	if (r) {
  		ti->error = "dm-cache: Cache device lookup failed";
  		goto bad2;
  	}
  
! 	dmc->io_client = dm_io_client_create(DMCACHE_COPY_PAGES);
  	if (IS_ERR(dmc->io_client)) {
  		r = PTR_ERR(dmc->io_client);
  		ti->error = "Failed to create io client\n";
  		goto bad3;
  	}
  
! 	r = dm_kcopyd_client_create(DMCACHE_COPY_PAGES, &dmc->kcp_client);
! 	if (r) {
  		ti->error = "Failed to initialize kcopyd client\n";
  		goto bad4;
  	}
***************
*** 1824,1830 ****
  			ti->error = "Failed to initialize kcached";
  			goto bad5;
  		}
- 	}
  
  	if (argc >= 3) {
  		if (sscanf(argv[2], "%u", &persistence) != 1) {
--- 1507,1512 ----
***************
*** 1833,1840 ****
  			goto bad6;
  		}
  	}
- 
- 	if(init_flag == 0) {
  		if (1 == persistence) {
  			if (load_metadata(dmc)) {
  				ti->error = "dm-cache: Invalid cache configuration";
--- 1515,1520 ----
***************
*** 1847,1866 ****
  			r = -EINVAL;
  			goto bad6;
  		}
- 	}else{
- 
- 		DMINFO("Add new entry :%d  (%luB per) mem for %llu-entry cache" \
- 				"associativity:%u, block size:%u " \
- 				"sectors(%uKB), %s) table %llu",ctn_dm_dev,
- 				(unsigned long) sizeof(struct cacheblock),
- 				(unsigned long long) dmc->size,
- 				dmc->assoc, dmc->block_size,
- 				dmc->block_size >> (10-SECTOR_SHIFT),
- 				dmc->write_policy ? "write-back" : "write-through",
- 				(unsigned long long)ti->table);
- 
- 		goto init_sc;
- 	}
  
  	if (argc >= 4) {
  		if (sscanf(argv[3], "%u", &dmc->block_size) != 1) {
--- 1527,1532 ----
***************
*** 1950,1956 ****
                 "sectors(%uKB), %s)",
                 (unsigned long long) order >> 10, (unsigned long) sizeof(struct cacheblock),
                 (unsigned long long) dmc->size,
!                (unsigned long long) dmc->size * dmc->block_size >> (20-SECTOR_SHIFT),
                 dmc->assoc, dmc->block_size,
                 dmc->block_size >> (10-SECTOR_SHIFT),
                 dmc->write_policy ? "write-back" : "write-through");
--- 1614,1620 ----
  	       "sectors(%uKB), %s)",
  	       (unsigned long long) order >> 10, (unsigned long) sizeof(struct cacheblock),
  	       (unsigned long long) dmc->size,
! 	       (unsigned long long) data_size >> (20-SECTOR_SHIFT),
  	       dmc->assoc, dmc->block_size,
  	       dmc->block_size >> (10-SECTOR_SHIFT),
  	       dmc->write_policy ? "write-back" : "write-through");
***************
*** 1967,1974 ****
  	for (i=0; i<dmc->size; i++) {
  		bio_list_init(&dmc->cache[i].bios);
  		if(!persistence) dmc->cache[i].state = 0;
- 		dmc->cache[i].state = 0;
- 		atomic_set(&dmc->cache[i].status, 0);
  		dmc->cache[i].counter = 0;
  		spin_lock_init(&dmc->cache[i].lock);
  	}
--- 1630,1635 ----
***************
*** 1981,1991 ****
  	dmc->replace = 0;
  	dmc->writeback = 0;
  	dmc->dirty = 0;
- init_sc:
- 	ti->split_io = dmc->block_size;
- 	ti->private = &virtual_mapping[ctn_dm_dev-1];
  
! 	init_flag = 1;
  	return 0;
  
  bad6:
--- 1642,1650 ----
  	dmc->replace = 0;
  	dmc->writeback = 0;
  	dmc->dirty = 0;
  
! 	ti->split_io = dmc->block_size;
! 	ti->private = dmc;
  	return 0;
  
  bad6:
***************
*** 1999,2011 ****
  bad2:
  	dm_put_device(ti, dmc->src_dev);
  bad1:
! 
  bad:
  	return r;
  }
  
  
! static void cache_flush(struct cache_c *dmc, int disk)
  {
  	struct cacheblock *cache = dmc->cache;
  	sector_t i = 0;
--- 1658,1670 ----
  bad2:
  	dm_put_device(ti, dmc->src_dev);
  bad1:
! 	kfree(dmc);
  bad:
  	return r;
  }
  
  
! static void cache_flush(struct cache_c *dmc)
  {
  	struct cacheblock *cache = dmc->cache;
  	sector_t i = 0;
***************
*** 2014,2020 ****
  	DMINFO("Flush dirty blocks (%llu) ...", (unsigned long long) dmc->dirty_blocks);
          while (i< dmc->size) {
  		j = 1;
! 		if (is_state(cache[i].state, DIRTY && cache[i].disk == disk)) {
  			while ((i+j) < dmc->size && is_state(cache[i+j].state, DIRTY)
  			       && (cache[i+j].block == cache[i].block + j *
  			       dmc->block_size)) {
--- 1673,1679 ----
  	DMINFO("Flush dirty blocks (%llu) ...", (unsigned long long) dmc->dirty_blocks);
  	while (i< dmc->size) {
  		j = 1;
! 		if (is_state(cache[i].state, DIRTY)) {
  			while ((i+j) < dmc->size && is_state(cache[i+j].state, DIRTY)
  			       && (cache[i+j].block == cache[i].block + j *
  			       dmc->block_size)) {
***************
*** 2032,2045 ****
   */
  static void cache_dtr(struct dm_target *ti)
  {
! 	struct cache_c *dmc = shared_cache;
! 	struct v_map *map_dev = (struct v_map *) ti->private;
! 	struct dm_dev_internal *dd;
! 	dd = container_of(dmc->cache_dev, struct dm_dev_internal,dm_dev);
  
! 	if (dmc->dirty_blocks > 0) cache_flush(dmc,map_dev->identifier);
  
- 	if(idx_dm_dev == 1){
  		kcached_client_destroy(dmc);
  
  		dm_kcopyd_client_destroy(dmc->kcp_client);
--- 1691,1700 ----
   */
  static void cache_dtr(struct dm_target *ti)
  {
! 	struct cache_c *dmc = (struct cache_c *) ti->private;
  
! 	if (dmc->dirty_blocks > 0) cache_flush(dmc);
  
  	kcached_client_destroy(dmc);
  
  	dm_kcopyd_client_destroy(dmc->kcp_client);
***************
*** 2056,2071 ****
  		vfree((void *)dmc->cache);
  		dm_io_client_destroy(dmc->io_client);
  
! 		blkdev_put(dd->dm_dev.bdev , dd->dm_dev.mode);
! 		kfree(dd);
! 		kfree(shared_cache);
! 		kfree(virtual_mapping);
! 		init_flag = 0;
! 	}
! 	
! 	map_dev->active = 0;
! 	dm_put_device(map_dev->ti,map_dev->src_dev);
! 	idx_dm_dev--;
  }
  
  /*
--- 1711,1719 ----
  	vfree((void *)dmc->cache);
  	dm_io_client_destroy(dmc->io_client);
  
! 	dm_put_device(ti, dmc->src_dev);
! 	dm_put_device(ti, dmc->cache_dev);
! 	kfree(dmc);
  }
  
  /*
***************
*** 2076,2082 ****
  static int cache_status(struct dm_target *ti, status_type_t type,
  			 char *result, unsigned int maxlen)
  {
! 	struct cache_c *dmc = shared_cache;
  	int sz = 0;
  
  	switch (type) {
--- 1724,1730 ----
  static int cache_status(struct dm_target *ti, status_type_t type,
  			 char *result, unsigned int maxlen)
  {
! 	struct cache_c *dmc = (struct cache_c *) ti->private;
  	int sz = 0;
  
  	switch (type) {
